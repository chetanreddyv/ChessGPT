{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Data preprocessing completed.\n",
      "Creating datasets...\n",
      "Datasets created.\n",
      "Data loaders initialized.\n",
      "Initializing model, loss function, and optimizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chetan/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, loss function, and optimizer initialized.\n",
      "Using device: cpu\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset (only first 5000 rows)\n",
    "final_df = pd.read_csv('final_chess_games.csv', nrows=5000)\n",
    "final_df = final_df[['Result', 'AN']]\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(df):\n",
    "    sequences = df['AN'].apply(lambda x: x.strip().split())\n",
    "    return sequences.tolist()\n",
    "\n",
    "print(\"Preprocessing data...\")\n",
    "sequences = preprocess_data(final_df)\n",
    "print(\"Data preprocessing completed.\")\n",
    "\n",
    "# Build vocabulary\n",
    "all_moves = set(move for seq in sequences for move in seq)\n",
    "vocab = {move: idx for idx, move in enumerate(all_moves)}\n",
    "vocab['<pad>'] = len(vocab)  # Add padding token\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_length = max(len(seq) for seq in sequences) - 1\n",
    "\n",
    "# Define the dataset\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, sequences, vocab, max_length):\n",
    "        self.sequences = sequences\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_seq = [self.vocab.get(move, self.vocab['<pad>']) for move in sequence[:-1]]\n",
    "        target_seq = [self.vocab.get(move, self.vocab['<pad>']) for move in sequence[1:]]\n",
    "\n",
    "        # Pad sequences\n",
    "        input_seq += [self.vocab['<pad>']] * (self.max_length - len(input_seq))\n",
    "        target_seq += [self.vocab['<pad>']] * (self.max_length - len(target_seq))\n",
    "\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "train_sequences, val_sequences = train_test_split(sequences, test_size=0.2, random_state=42)\n",
    "train_dataset = ChessDataset(train_sequences, vocab, max_length)\n",
    "val_dataset = ChessDataset(val_sequences, vocab, max_length)\n",
    "print(\"Datasets created.\")\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 8  # Adjust batch size if necessary\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"Data loaders initialized.\")\n",
    "\n",
    "# Define positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(-math.log(10000.0) * torch.arange(0, d_model, 2).float() / d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "print(\"Initializing model, loss function, and optimizer...\")\n",
    "model = TransformerModel(vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"Model, loss function, and optimizer initialized.\")\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10\n",
    "checkpoint_interval = 2  # Save checkpoint every 2 epochs\n",
    "accumulation_steps = 4  # Number of batches to accumulate gradients\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "        batch_start_time = time.time()\n",
    "        src, tgt = src.transpose(0, 1).to(device), tgt.transpose(0, 1).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(src, tgt[:-1, :])\n",
    "        loss = criterion(output.view(-1, vocab_size), tgt[1:, :].reshape(-1))\n",
    "        loss = 0.5 * loss  # Scale the loss by half\n",
    "\n",
    "        # Backward pass\n",
    "        (loss / accumulation_steps).backward()\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch_elapsed_time = time.time() - batch_start_time\n",
    "        print(f\"Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}, Batch Time: {batch_elapsed_time:.2f}s\")\n",
    "\n",
    "    epoch_elapsed_time = time.time() - epoch_start_time\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} training completed. Average Loss: {avg_loss:.4f}. Elapsed Time: {epoch_elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Validation step\n",
    "    validation_start_time = time.time()\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.transpose(0, 1).to(device), tgt.transpose(0, 1).to(device)\n",
    "\n",
    "            output = model(src, tgt[:-1, :])\n",
    "            loss = criterion(output.view(-1, vocab_size), tgt[1:, :].reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    validation_elapsed_time = time.time() - validation_start_time\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}. Validation Time: {validation_elapsed_time:.2f} seconds\")\n",
    "\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        checkpoint_path = f'transformer_model_epoch_{epoch+1}.pth'\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), 'transformer_model_final.pth')\n",
    "print(\"Training completed and model saved.\")\n",
    "\n",
    "# Function to generate moves\n",
    "def generate_moves(model, start_sequence, vocab, max_length=50):\n",
    "    model.eval()\n",
    "    generated_moves = start_sequence.copy()\n",
    "    input_seq = [vocab.get(move, vocab['<pad>']) for move in generated_moves]\n",
    "    input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(1).to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        tgt_input = input_tensor[-1:, :]\n",
    "        output = model(input_tensor, tgt_input)\n",
    "        next_move_idx = output.argmax(dim=-1)[-1, 0].item()\n",
    "        next_move = [move for move, idx in vocab.items() if idx == next_move_idx][0]\n",
    "        generated_moves.append(next_move)\n",
    "\n",
    "        if next_move == '<pad>':\n",
    "            break\n",
    "\n",
    "        input_seq.append(next_move_idx)\n",
    "        input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(1).to(device)\n",
    "\n",
    "    return generated_moves\n",
    "\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load('transformer_model_final.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# Example usage\n",
    "start_sequence = ['1.', 'e4', 'e5', '2.', 'Nf3', 'Nc6']\n",
    "generated_moves = generate_moves(model, start_sequence, vocab)\n",
    "print(\"Generated moves:\", ' '.join(generated_moves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Data preprocessing completed.\n",
      "Creating datasets...\n",
      "Datasets created.\n",
      "Data loaders initialized.\n",
      "Initializing model, loss function, and optimizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chetan/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, loss function, and optimizer initialized.\n",
      "Using device: cpu\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset (only first 5000 rows)\n",
    "final_df = pd.read_csv('final_chess_games.csv', nrows=5000)\n",
    "final_df = final_df[['Result', 'AN']]\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(df):\n",
    "    sequences = df['AN'].apply(lambda x: x.strip().split())\n",
    "    return sequences.tolist()\n",
    "\n",
    "print(\"Preprocessing data...\")\n",
    "sequences = preprocess_data(final_df)\n",
    "print(\"Data preprocessing completed.\")\n",
    "\n",
    "# Build vocabulary\n",
    "all_moves = set(move for seq in sequences for move in seq)\n",
    "vocab = {move: idx for idx, move in enumerate(all_moves)}\n",
    "vocab['<pad>'] = len(vocab)  # Add padding token\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_length = max(len(seq) for seq in sequences) - 1\n",
    "\n",
    "# Define the dataset\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, sequences, vocab, max_length):\n",
    "        self.sequences = sequences\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_seq = [self.vocab.get(move, self.vocab['<pad>']) for move in sequence[:-1]]\n",
    "        target_seq = [self.vocab.get(move, self.vocab['<pad>']) for move in sequence[1:]]\n",
    "\n",
    "        # Pad sequences\n",
    "        input_seq += [self.vocab['<pad>']] * (self.max_length - len(input_seq))\n",
    "        target_seq += [self.vocab['<pad>']] * (self.max_length - len(target_seq))\n",
    "\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "train_sequences, val_sequences = train_test_split(sequences, test_size=0.2, random_state=42)\n",
    "train_dataset = ChessDataset(train_sequences, vocab, max_length)\n",
    "val_dataset = ChessDataset(val_sequences, vocab, max_length)\n",
    "print(\"Datasets created.\")\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 8  # Adjust batch size if necessary\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"Data loaders initialized.\")\n",
    "\n",
    "# Define positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(-math.log(10000.0) * torch.arange(0, d_model, 2).float() / d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "print(\"Initializing model, loss function, and optimizer...\")\n",
    "model = TransformerModel(vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"Model, loss function, and optimizer initialized.\")\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10\n",
    "checkpoint_interval = 2  # Save checkpoint every 2 epochs\n",
    "accumulation_steps = 4  # Number of batches to accumulate gradients\n",
    "patience = 3  # Early stopping patience\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "        batch_start_time = time.time()\n",
    "        src, tgt = src.transpose(0, 1).to(device), tgt.transpose(0, 1).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(src, tgt[:-1, :])\n",
    "        loss = criterion(output.view(-1, vocab_size), tgt[1:, :].reshape(-1))\n",
    "        loss = 0.5 * loss  # Scale the loss by half\n",
    "\n",
    "        # Backward pass\n",
    "        (loss / accumulation_steps).backward()\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch_elapsed_time = time.time() - batch_start_time\n",
    "        print(f\"Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}, Batch Time: {batch_elapsed_time:.2f}s\")\n",
    "\n",
    "    epoch_elapsed_time = time.time() - epoch_start_time\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} training completed. Average Loss: {avg_loss:.4f}. Elapsed Time: {epoch_elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Validation step\n",
    "    validation_start_time = time.time()\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.transpose(0, 1).to(device), tgt.transpose(0, 1).to(device)\n",
    "\n",
    "            output = model(src, tgt[:-1, :])\n",
    "            loss = criterion(output.view(-1, vocab_size), tgt[1:, :].reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    validation_elapsed_time = time.time() - validation_start_time\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}. Validation Time: {validation_elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Validation loss improved. Model saved.\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"No improvement in validation loss for {counter} epoch(s).\")\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        checkpoint_path = f'transformer_model_epoch_{epoch+1}.pth'\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "print(\"Training completed and best model loaded.\")\n",
    "\n",
    "# Function to generate moves\n",
    "def generate_moves(model, start_sequence, vocab, max_length=50):\n",
    "    model.eval()\n",
    "    generated_moves = start_sequence.copy()\n",
    "    input_seq = [vocab.get(move, vocab['<pad>']) for move in generated_moves]\n",
    "    input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(1).to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        tgt_input = input_tensor[-1:, :]\n",
    "        output = model(input_tensor, tgt_input)\n",
    "        next_move_idx = output.argmax(dim=-1)[-1, 0].item()\n",
    "        next_move = [move for move, idx in vocab.items() if idx == next_move_idx][0]\n",
    "        generated_moves.append(next_move)\n",
    "\n",
    "        if next_move == '<pad>':\n",
    "            break\n",
    "\n",
    "        input_seq.append(next_move_idx)\n",
    "        input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(1).to(device)\n",
    "\n",
    "    return generated_moves\n",
    "\n",
    "# Example usage\n",
    "start_sequence = ['1.', 'e4', 'e5', '2.', 'Nf3', 'Nc6']\n",
    "generated_moves = generate_moves(model, start_sequence, vocab)\n",
    "print(\"Generated moves:\", ' '.join(generated_moves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
