{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Data preprocessing completed.\n",
      "Creating datasets...\n",
      "Datasets created.\n",
      "Data loaders initialized.\n",
      "Initializing model, loss function, and optimizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chetan/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, loss function, and optimizer initialized.\n",
      "Using device: cpu\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset (only first 5000 rows)\n",
    "final_df = pd.read_csv('final_chess_games.csv', nrows=5000)\n",
    "final_df = final_df[['Result', 'AN']]\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(df):\n",
    "    sequences = df['AN'].apply(lambda x: x.strip().split())\n",
    "    return sequences.tolist()\n",
    "\n",
    "print(\"Preprocessing data...\")\n",
    "sequences = preprocess_data(final_df)\n",
    "print(\"Data preprocessing completed.\")\n",
    "\n",
    "# Build vocabulary\n",
    "all_moves = set(move for seq in sequences for move in seq)\n",
    "vocab = {move: idx for idx, move in enumerate(all_moves)}\n",
    "vocab['<pad>'] = len(vocab)  # Add padding token\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_length = max(len(seq) for seq in sequences) - 1\n",
    "\n",
    "# Define the dataset\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, sequences, vocab, max_length):\n",
    "        self.sequences = sequences\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_seq = [self.vocab.get(move, self.vocab['<pad>']) for move in sequence[:-1]]\n",
    "        target_seq = [self.vocab.get(move, self.vocab['<pad>']) for move in sequence[1:]]\n",
    "\n",
    "        # Pad sequences\n",
    "        input_seq += [self.vocab['<pad>']] * (self.max_length - len(input_seq))\n",
    "        target_seq += [self.vocab['<pad>']] * (self.max_length - len(target_seq))\n",
    "\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "train_sequences, val_sequences = train_test_split(sequences, test_size=0.2, random_state=42)\n",
    "train_dataset = ChessDataset(train_sequences, vocab, max_length)\n",
    "val_dataset = ChessDataset(val_sequences, vocab, max_length)\n",
    "print(\"Datasets created.\")\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 8  # Adjust batch size if necessary\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"Data loaders initialized.\")\n",
    "\n",
    "# Define positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(-math.log(10000.0) * torch.arange(0, d_model, 2).float() / d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "print(\"Initializing model, loss function, and optimizer...\")\n",
    "model = TransformerModel(vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"Model, loss function, and optimizer initialized.\")\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10\n",
    "checkpoint_interval = 2  # Save checkpoint every 2 epochs\n",
    "accumulation_steps = 4  # Number of batches to accumulate gradients\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "        batch_start_time = time.time()\n",
    "        src, tgt = src.transpose(0, 1).to(device), tgt.transpose(0, 1).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(src, tgt[:-1, :])\n",
    "        loss = criterion(output.view(-1, vocab_size), tgt[1:, :].reshape(-1))\n",
    "        loss = 0.5 * loss  # Scale the loss by half\n",
    "\n",
    "        # Backward pass\n",
    "        (loss / accumulation_steps).backward()\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch_elapsed_time = time.time() - batch_start_time\n",
    "        print(f\"Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}, Batch Time: {batch_elapsed_time:.2f}s\")\n",
    "\n",
    "    epoch_elapsed_time = time.time() - epoch_start_time\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} training completed. Average Loss: {avg_loss:.4f}. Elapsed Time: {epoch_elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Validation step\n",
    "    validation_start_time = time.time()\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.transpose(0, 1).to(device), tgt.transpose(0, 1).to(device)\n",
    "\n",
    "            output = model(src, tgt[:-1, :])\n",
    "            loss = criterion(output.view(-1, vocab_size), tgt[1:, :].reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    validation_elapsed_time = time.time() - validation_start_time\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}. Validation Time: {validation_elapsed_time:.2f} seconds\")\n",
    "\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        checkpoint_path = f'transformer_model_epoch_{epoch+1}.pth'\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), 'transformer_model_final.pth')\n",
    "print(\"Training completed and model saved.\")\n",
    "\n",
    "# Function to generate moves\n",
    "def generate_moves(model, start_sequence, vocab, max_length=50):\n",
    "    model.eval()\n",
    "    generated_moves = start_sequence.copy()\n",
    "    input_seq = [vocab.get(move, vocab['<pad>']) for move in generated_moves]\n",
    "    input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(1).to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        tgt_input = input_tensor[-1:, :]\n",
    "        output = model(input_tensor, tgt_input)\n",
    "        next_move_idx = output.argmax(dim=-1)[-1, 0].item()\n",
    "        next_move = [move for move, idx in vocab.items() if idx == next_move_idx][0]\n",
    "        generated_moves.append(next_move)\n",
    "\n",
    "        if next_move == '<pad>':\n",
    "            break\n",
    "\n",
    "        input_seq.append(next_move_idx)\n",
    "        input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(1).to(device)\n",
    "\n",
    "    return generated_moves\n",
    "\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load('transformer_model_final.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# Example usage\n",
    "start_sequence = ['1.', 'e4', 'e5', '2.', 'Nf3', 'Nc6']\n",
    "generated_moves = generate_moves(model, start_sequence, vocab)\n",
    "print(\"Generated moves:\", ' '.join(generated_moves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Data preprocessing completed.\n",
      "Creating datasets...\n",
      "Datasets created.\n",
      "Data loaders initialized.\n",
      "Initializing model, loss function, and optimizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chetan/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, loss function, and optimizer initialized.\n",
      "Using device: cpu\n",
      "Starting training...\n",
      "Epoch 1, Batch 1/500, Loss: 4.6904, Batch Time: 67.42s\n",
      "Epoch 1, Batch 2/500, Loss: 4.6887, Batch Time: 59.39s\n",
      "Epoch 1, Batch 3/500, Loss: 4.6893, Batch Time: 61.46s\n",
      "Epoch 1, Batch 4/500, Loss: 4.6877, Batch Time: 58.69s\n",
      "Epoch 1, Batch 5/500, Loss: 3.7628, Batch Time: 57.88s\n",
      "Epoch 1, Batch 6/500, Loss: 4.5450, Batch Time: 58.14s\n",
      "Epoch 1, Batch 7/500, Loss: 3.9541, Batch Time: 62.45s\n",
      "Epoch 1, Batch 8/500, Loss: 3.7556, Batch Time: 61.65s\n",
      "Epoch 1, Batch 9/500, Loss: 3.4956, Batch Time: 61.43s\n",
      "Epoch 1, Batch 10/500, Loss: 3.4861, Batch Time: 64.23s\n",
      "Epoch 1, Batch 11/500, Loss: 3.3861, Batch Time: 59.39s\n",
      "Epoch 1, Batch 12/500, Loss: 4.3505, Batch Time: 60.40s\n",
      "Epoch 1, Batch 13/500, Loss: 3.4199, Batch Time: 59.21s\n",
      "Epoch 1, Batch 14/500, Loss: 4.1430, Batch Time: 60.53s\n",
      "Epoch 1, Batch 15/500, Loss: 3.4306, Batch Time: 63.20s\n",
      "Epoch 1, Batch 16/500, Loss: 3.7551, Batch Time: 64.35s\n",
      "Epoch 1, Batch 17/500, Loss: 3.9011, Batch Time: 64.76s\n",
      "Epoch 1, Batch 18/500, Loss: 3.6102, Batch Time: 63.36s\n",
      "Epoch 1, Batch 19/500, Loss: 3.5627, Batch Time: 64.88s\n",
      "Epoch 1, Batch 20/500, Loss: 3.4671, Batch Time: 64.00s\n",
      "Epoch 1, Batch 21/500, Loss: 3.1642, Batch Time: 61.01s\n",
      "Epoch 1, Batch 22/500, Loss: 3.0176, Batch Time: 62.00s\n",
      "Epoch 1, Batch 23/500, Loss: 3.2264, Batch Time: 63.31s\n",
      "Epoch 1, Batch 24/500, Loss: 3.2367, Batch Time: 68.08s\n",
      "Epoch 1, Batch 25/500, Loss: 3.3486, Batch Time: 61.49s\n",
      "Epoch 1, Batch 26/500, Loss: 3.4803, Batch Time: 70.19s\n",
      "Epoch 1, Batch 27/500, Loss: 3.4862, Batch Time: 58.63s\n",
      "Epoch 1, Batch 28/500, Loss: 3.6249, Batch Time: 58.88s\n",
      "Epoch 1, Batch 29/500, Loss: 3.1961, Batch Time: 58.77s\n",
      "Epoch 1, Batch 30/500, Loss: 3.1847, Batch Time: 60.41s\n",
      "Epoch 1, Batch 31/500, Loss: 3.1245, Batch Time: 60.27s\n",
      "Epoch 1, Batch 32/500, Loss: 3.3332, Batch Time: 60.12s\n",
      "Epoch 1, Batch 33/500, Loss: 3.0723, Batch Time: 57.62s\n",
      "Epoch 1, Batch 34/500, Loss: 3.1450, Batch Time: 59.73s\n",
      "Epoch 1, Batch 35/500, Loss: 3.2147, Batch Time: 58.77s\n",
      "Epoch 1, Batch 36/500, Loss: 2.9744, Batch Time: 58.61s\n",
      "Epoch 1, Batch 37/500, Loss: 3.2658, Batch Time: 58.10s\n",
      "Epoch 1, Batch 38/500, Loss: 3.2278, Batch Time: 59.60s\n",
      "Epoch 1, Batch 39/500, Loss: 3.0131, Batch Time: 56.92s\n",
      "Epoch 1, Batch 40/500, Loss: 3.2138, Batch Time: 59.67s\n",
      "Epoch 1, Batch 41/500, Loss: 3.0378, Batch Time: 59.64s\n",
      "Epoch 1, Batch 42/500, Loss: 3.0234, Batch Time: 58.92s\n",
      "Epoch 1, Batch 43/500, Loss: 2.8687, Batch Time: 57.93s\n",
      "Epoch 1, Batch 44/500, Loss: 3.1838, Batch Time: 60.50s\n",
      "Epoch 1, Batch 45/500, Loss: 3.3088, Batch Time: 58.30s\n",
      "Epoch 1, Batch 46/500, Loss: 3.2252, Batch Time: 58.85s\n",
      "Epoch 1, Batch 47/500, Loss: 2.9992, Batch Time: 60.18s\n",
      "Epoch 1, Batch 48/500, Loss: 2.8872, Batch Time: 60.09s\n",
      "Epoch 1, Batch 49/500, Loss: 3.0713, Batch Time: 59.67s\n",
      "Epoch 1, Batch 50/500, Loss: 3.1023, Batch Time: 57.35s\n",
      "Epoch 1, Batch 51/500, Loss: 3.2196, Batch Time: 57.31s\n",
      "Epoch 1, Batch 52/500, Loss: 3.0614, Batch Time: 58.66s\n",
      "Epoch 1, Batch 53/500, Loss: 3.5054, Batch Time: 58.78s\n",
      "Epoch 1, Batch 54/500, Loss: 3.3535, Batch Time: 60.65s\n",
      "Epoch 1, Batch 55/500, Loss: 3.0546, Batch Time: 59.93s\n",
      "Epoch 1, Batch 56/500, Loss: 2.9774, Batch Time: 58.04s\n",
      "Epoch 1, Batch 57/500, Loss: 2.9859, Batch Time: 58.89s\n",
      "Epoch 1, Batch 58/500, Loss: 3.2804, Batch Time: 60.23s\n",
      "Epoch 1, Batch 59/500, Loss: 3.1257, Batch Time: 58.28s\n",
      "Epoch 1, Batch 60/500, Loss: 2.8259, Batch Time: 59.03s\n",
      "Epoch 1, Batch 61/500, Loss: 2.8488, Batch Time: 58.82s\n",
      "Epoch 1, Batch 62/500, Loss: 3.1623, Batch Time: 58.49s\n",
      "Epoch 1, Batch 63/500, Loss: 2.9610, Batch Time: 59.53s\n",
      "Epoch 1, Batch 64/500, Loss: 3.2525, Batch Time: 59.00s\n",
      "Epoch 1, Batch 65/500, Loss: 3.3595, Batch Time: 60.57s\n",
      "Epoch 1, Batch 66/500, Loss: 3.4325, Batch Time: 57.82s\n",
      "Epoch 1, Batch 67/500, Loss: 3.0396, Batch Time: 58.05s\n",
      "Epoch 1, Batch 68/500, Loss: 3.1158, Batch Time: 59.99s\n",
      "Epoch 1, Batch 69/500, Loss: 3.0251, Batch Time: 60.46s\n",
      "Epoch 1, Batch 70/500, Loss: 2.9645, Batch Time: 58.93s\n",
      "Epoch 1, Batch 71/500, Loss: 3.2433, Batch Time: 59.53s\n",
      "Epoch 1, Batch 72/500, Loss: 3.0605, Batch Time: 60.97s\n",
      "Epoch 1, Batch 73/500, Loss: 2.9953, Batch Time: 58.41s\n",
      "Epoch 1, Batch 74/500, Loss: 3.3871, Batch Time: 59.98s\n",
      "Epoch 1, Batch 75/500, Loss: 3.1261, Batch Time: 59.67s\n",
      "Epoch 1, Batch 76/500, Loss: 3.1076, Batch Time: 59.88s\n",
      "Epoch 1, Batch 77/500, Loss: 3.1980, Batch Time: 58.81s\n",
      "Epoch 1, Batch 78/500, Loss: 3.0056, Batch Time: 59.66s\n",
      "Epoch 1, Batch 79/500, Loss: 2.9203, Batch Time: 58.71s\n",
      "Epoch 1, Batch 80/500, Loss: 3.3476, Batch Time: 59.71s\n",
      "Epoch 1, Batch 81/500, Loss: 3.1935, Batch Time: 59.06s\n",
      "Epoch 1, Batch 82/500, Loss: 3.1340, Batch Time: 60.44s\n",
      "Epoch 1, Batch 83/500, Loss: 3.1118, Batch Time: 60.28s\n",
      "Epoch 1, Batch 84/500, Loss: 3.0174, Batch Time: 59.77s\n",
      "Epoch 1, Batch 85/500, Loss: 2.8602, Batch Time: 59.02s\n",
      "Epoch 1, Batch 86/500, Loss: 3.2787, Batch Time: 60.50s\n",
      "Epoch 1, Batch 87/500, Loss: 2.8425, Batch Time: 60.76s\n",
      "Epoch 1, Batch 88/500, Loss: 2.8911, Batch Time: 59.74s\n",
      "Epoch 1, Batch 89/500, Loss: 2.9011, Batch Time: 59.16s\n",
      "Epoch 1, Batch 90/500, Loss: 2.9548, Batch Time: 61.89s\n",
      "Epoch 1, Batch 91/500, Loss: 3.3702, Batch Time: 62.03s\n",
      "Epoch 1, Batch 92/500, Loss: 3.1351, Batch Time: 59.17s\n",
      "Epoch 1, Batch 93/500, Loss: 2.9191, Batch Time: 58.64s\n",
      "Epoch 1, Batch 94/500, Loss: 3.1626, Batch Time: 57.79s\n",
      "Epoch 1, Batch 95/500, Loss: 3.1306, Batch Time: 61.34s\n",
      "Epoch 1, Batch 96/500, Loss: 3.2860, Batch Time: 59.94s\n",
      "Epoch 1, Batch 97/500, Loss: 2.7843, Batch Time: 58.76s\n",
      "Epoch 1, Batch 98/500, Loss: 2.8710, Batch Time: 59.20s\n",
      "Epoch 1, Batch 99/500, Loss: 3.2097, Batch Time: 58.91s\n",
      "Epoch 1, Batch 100/500, Loss: 3.2858, Batch Time: 60.35s\n",
      "Epoch 1, Batch 101/500, Loss: 3.0442, Batch Time: 61.10s\n",
      "Epoch 1, Batch 102/500, Loss: 2.9239, Batch Time: 58.84s\n",
      "Epoch 1, Batch 103/500, Loss: 3.4255, Batch Time: 60.01s\n",
      "Epoch 1, Batch 104/500, Loss: 3.2510, Batch Time: 58.82s\n",
      "Epoch 1, Batch 105/500, Loss: 3.2418, Batch Time: 59.77s\n",
      "Epoch 1, Batch 106/500, Loss: 3.3546, Batch Time: 60.46s\n",
      "Epoch 1, Batch 107/500, Loss: 2.9294, Batch Time: 60.30s\n",
      "Epoch 1, Batch 108/500, Loss: 3.4461, Batch Time: 60.40s\n",
      "Epoch 1, Batch 109/500, Loss: 2.9857, Batch Time: 60.53s\n",
      "Epoch 1, Batch 110/500, Loss: 3.3111, Batch Time: 58.43s\n",
      "Epoch 1, Batch 111/500, Loss: 2.7771, Batch Time: 60.28s\n",
      "Epoch 1, Batch 112/500, Loss: 3.3481, Batch Time: 58.98s\n",
      "Epoch 1, Batch 113/500, Loss: 2.9984, Batch Time: 58.60s\n",
      "Epoch 1, Batch 114/500, Loss: 3.0849, Batch Time: 59.76s\n",
      "Epoch 1, Batch 115/500, Loss: 3.0574, Batch Time: 59.90s\n",
      "Epoch 1, Batch 116/500, Loss: 2.9607, Batch Time: 60.84s\n",
      "Epoch 1, Batch 117/500, Loss: 3.1977, Batch Time: 58.79s\n",
      "Epoch 1, Batch 118/500, Loss: 3.0489, Batch Time: 58.10s\n",
      "Epoch 1, Batch 119/500, Loss: 3.0570, Batch Time: 59.28s\n",
      "Epoch 1, Batch 120/500, Loss: 3.1736, Batch Time: 60.39s\n",
      "Epoch 1, Batch 121/500, Loss: 3.0475, Batch Time: 61.98s\n",
      "Epoch 1, Batch 122/500, Loss: 3.2361, Batch Time: 59.59s\n",
      "Epoch 1, Batch 123/500, Loss: 3.1638, Batch Time: 58.42s\n",
      "Epoch 1, Batch 124/500, Loss: 3.0229, Batch Time: 58.75s\n",
      "Epoch 1, Batch 125/500, Loss: 2.9484, Batch Time: 59.60s\n",
      "Epoch 1, Batch 126/500, Loss: 3.0141, Batch Time: 58.71s\n",
      "Epoch 1, Batch 127/500, Loss: 3.0852, Batch Time: 57.65s\n",
      "Epoch 1, Batch 128/500, Loss: 3.2266, Batch Time: 60.36s\n",
      "Epoch 1, Batch 129/500, Loss: 2.9963, Batch Time: 59.17s\n",
      "Epoch 1, Batch 130/500, Loss: 2.9823, Batch Time: 59.65s\n",
      "Epoch 1, Batch 131/500, Loss: 3.2547, Batch Time: 57.95s\n",
      "Epoch 1, Batch 132/500, Loss: 3.0069, Batch Time: 58.65s\n",
      "Epoch 1, Batch 133/500, Loss: 3.2992, Batch Time: 59.61s\n",
      "Epoch 1, Batch 134/500, Loss: 3.1422, Batch Time: 58.99s\n",
      "Epoch 1, Batch 135/500, Loss: 3.2086, Batch Time: 57.38s\n",
      "Epoch 1, Batch 136/500, Loss: 3.2248, Batch Time: 59.73s\n",
      "Epoch 1, Batch 137/500, Loss: 3.1610, Batch Time: 58.74s\n",
      "Epoch 1, Batch 138/500, Loss: 2.9525, Batch Time: 59.09s\n",
      "Epoch 1, Batch 139/500, Loss: 2.9204, Batch Time: 59.76s\n",
      "Epoch 1, Batch 140/500, Loss: 2.8768, Batch Time: 60.41s\n",
      "Epoch 1, Batch 141/500, Loss: 3.1637, Batch Time: 58.24s\n",
      "Epoch 1, Batch 142/500, Loss: 3.2136, Batch Time: 58.13s\n",
      "Epoch 1, Batch 143/500, Loss: 2.9453, Batch Time: 59.29s\n",
      "Epoch 1, Batch 144/500, Loss: 3.2693, Batch Time: 59.56s\n",
      "Epoch 1, Batch 145/500, Loss: 3.2394, Batch Time: 56.97s\n",
      "Epoch 1, Batch 146/500, Loss: 3.0466, Batch Time: 60.16s\n",
      "Epoch 1, Batch 147/500, Loss: 2.9109, Batch Time: 57.95s\n",
      "Epoch 1, Batch 148/500, Loss: 3.0304, Batch Time: 59.12s\n",
      "Epoch 1, Batch 149/500, Loss: 2.9902, Batch Time: 59.00s\n",
      "Epoch 1, Batch 150/500, Loss: 3.0931, Batch Time: 58.10s\n",
      "Epoch 1, Batch 151/500, Loss: 3.1842, Batch Time: 58.47s\n",
      "Epoch 1, Batch 152/500, Loss: 3.3382, Batch Time: 60.51s\n",
      "Epoch 1, Batch 153/500, Loss: 3.0416, Batch Time: 59.81s\n",
      "Epoch 1, Batch 154/500, Loss: 3.1307, Batch Time: 59.25s\n",
      "Epoch 1, Batch 155/500, Loss: 2.8964, Batch Time: 59.43s\n",
      "Epoch 1, Batch 156/500, Loss: 2.9071, Batch Time: 59.52s\n",
      "Epoch 1, Batch 157/500, Loss: 3.0485, Batch Time: 59.22s\n",
      "Epoch 1, Batch 158/500, Loss: 2.8432, Batch Time: 58.89s\n",
      "Epoch 1, Batch 159/500, Loss: 2.9884, Batch Time: 59.17s\n",
      "Epoch 1, Batch 160/500, Loss: 3.2992, Batch Time: 59.30s\n",
      "Epoch 1, Batch 161/500, Loss: 3.2461, Batch Time: 60.31s\n",
      "Epoch 1, Batch 162/500, Loss: 2.8499, Batch Time: 57.84s\n",
      "Epoch 1, Batch 163/500, Loss: 2.9901, Batch Time: 58.24s\n",
      "Epoch 1, Batch 164/500, Loss: 3.1707, Batch Time: 60.20s\n",
      "Epoch 1, Batch 165/500, Loss: 3.0082, Batch Time: 60.09s\n",
      "Epoch 1, Batch 166/500, Loss: 3.3628, Batch Time: 59.00s\n",
      "Epoch 1, Batch 167/500, Loss: 2.9461, Batch Time: 60.19s\n",
      "Epoch 1, Batch 168/500, Loss: 3.1171, Batch Time: 61.02s\n",
      "Epoch 1, Batch 169/500, Loss: 2.9723, Batch Time: 59.11s\n",
      "Epoch 1, Batch 170/500, Loss: 3.2970, Batch Time: 58.73s\n",
      "Epoch 1, Batch 171/500, Loss: 2.9747, Batch Time: 59.31s\n",
      "Epoch 1, Batch 172/500, Loss: 2.9964, Batch Time: 59.22s\n",
      "Epoch 1, Batch 173/500, Loss: 3.3654, Batch Time: 59.13s\n",
      "Epoch 1, Batch 174/500, Loss: 2.9476, Batch Time: 59.01s\n",
      "Epoch 1, Batch 175/500, Loss: 2.9669, Batch Time: 57.17s\n",
      "Epoch 1, Batch 176/500, Loss: 3.2661, Batch Time: 58.60s\n",
      "Epoch 1, Batch 177/500, Loss: 3.2437, Batch Time: 58.73s\n",
      "Epoch 1, Batch 178/500, Loss: 3.0747, Batch Time: 58.21s\n",
      "Epoch 1, Batch 179/500, Loss: 2.9834, Batch Time: 60.83s\n",
      "Epoch 1, Batch 180/500, Loss: 3.1162, Batch Time: 63.15s\n",
      "Epoch 1, Batch 181/500, Loss: 2.8927, Batch Time: 61.58s\n",
      "Epoch 1, Batch 182/500, Loss: 3.1469, Batch Time: 58.78s\n",
      "Epoch 1, Batch 183/500, Loss: 2.8821, Batch Time: 57.99s\n",
      "Epoch 1, Batch 184/500, Loss: 3.0723, Batch Time: 58.58s\n",
      "Epoch 1, Batch 185/500, Loss: 2.9492, Batch Time: 59.18s\n",
      "Epoch 1, Batch 186/500, Loss: 3.3852, Batch Time: 59.48s\n",
      "Epoch 1, Batch 187/500, Loss: 3.0061, Batch Time: 58.08s\n",
      "Epoch 1, Batch 188/500, Loss: 3.1112, Batch Time: 59.29s\n",
      "Epoch 1, Batch 189/500, Loss: 3.3167, Batch Time: 56.98s\n",
      "Epoch 1, Batch 190/500, Loss: 3.2871, Batch Time: 57.14s\n",
      "Epoch 1, Batch 191/500, Loss: 3.3811, Batch Time: 57.92s\n",
      "Epoch 1, Batch 192/500, Loss: 2.7285, Batch Time: 58.20s\n",
      "Epoch 1, Batch 193/500, Loss: 3.0514, Batch Time: 58.08s\n",
      "Epoch 1, Batch 194/500, Loss: 2.8540, Batch Time: 63.57s\n",
      "Epoch 1, Batch 195/500, Loss: 3.0420, Batch Time: 60.94s\n",
      "Epoch 1, Batch 196/500, Loss: 3.1053, Batch Time: 61.15s\n",
      "Epoch 1, Batch 197/500, Loss: 3.0075, Batch Time: 58.23s\n",
      "Epoch 1, Batch 198/500, Loss: 3.1152, Batch Time: 57.45s\n",
      "Epoch 1, Batch 199/500, Loss: 3.2542, Batch Time: 57.80s\n",
      "Epoch 1, Batch 200/500, Loss: 3.2263, Batch Time: 60.39s\n",
      "Epoch 1, Batch 201/500, Loss: 3.0485, Batch Time: 58.10s\n",
      "Epoch 1, Batch 202/500, Loss: 3.1999, Batch Time: 57.46s\n",
      "Epoch 1, Batch 203/500, Loss: 3.0606, Batch Time: 58.66s\n",
      "Epoch 1, Batch 204/500, Loss: 2.8172, Batch Time: 59.35s\n",
      "Epoch 1, Batch 205/500, Loss: 2.8946, Batch Time: 57.85s\n",
      "Epoch 1, Batch 206/500, Loss: 3.3763, Batch Time: 58.90s\n",
      "Epoch 1, Batch 207/500, Loss: 3.0304, Batch Time: 58.89s\n",
      "Epoch 1, Batch 208/500, Loss: 3.2154, Batch Time: 59.84s\n",
      "Epoch 1, Batch 209/500, Loss: 2.8948, Batch Time: 58.42s\n",
      "Epoch 1, Batch 210/500, Loss: 3.2942, Batch Time: 60.89s\n",
      "Epoch 1, Batch 211/500, Loss: 3.1540, Batch Time: 62.22s\n",
      "Epoch 1, Batch 212/500, Loss: 3.3000, Batch Time: 59.39s\n",
      "Epoch 1, Batch 213/500, Loss: 3.1056, Batch Time: 59.26s\n",
      "Epoch 1, Batch 214/500, Loss: 3.0669, Batch Time: 58.58s\n",
      "Epoch 1, Batch 215/500, Loss: 3.3800, Batch Time: 58.61s\n",
      "Epoch 1, Batch 216/500, Loss: 2.9757, Batch Time: 60.00s\n",
      "Epoch 1, Batch 217/500, Loss: 3.2317, Batch Time: 58.56s\n",
      "Epoch 1, Batch 218/500, Loss: 2.9321, Batch Time: 60.25s\n",
      "Epoch 1, Batch 219/500, Loss: 3.0279, Batch Time: 59.83s\n",
      "Epoch 1, Batch 220/500, Loss: 3.3328, Batch Time: 60.19s\n",
      "Epoch 1, Batch 221/500, Loss: 3.0179, Batch Time: 59.87s\n",
      "Epoch 1, Batch 222/500, Loss: 2.9845, Batch Time: 59.47s\n",
      "Epoch 1, Batch 223/500, Loss: 3.1267, Batch Time: 58.20s\n",
      "Epoch 1, Batch 224/500, Loss: 3.2874, Batch Time: 58.64s\n",
      "Epoch 1, Batch 225/500, Loss: 3.0829, Batch Time: 58.14s\n",
      "Epoch 1, Batch 226/500, Loss: 3.1688, Batch Time: 58.28s\n",
      "Epoch 1, Batch 227/500, Loss: 3.0586, Batch Time: 58.79s\n",
      "Epoch 1, Batch 228/500, Loss: 3.2327, Batch Time: 62.61s\n",
      "Epoch 1, Batch 229/500, Loss: 3.1180, Batch Time: 62.23s\n",
      "Epoch 1, Batch 230/500, Loss: 3.1847, Batch Time: 63.87s\n",
      "Epoch 1, Batch 231/500, Loss: 3.0423, Batch Time: 61.02s\n",
      "Epoch 1, Batch 232/500, Loss: 2.9653, Batch Time: 59.89s\n",
      "Epoch 1, Batch 233/500, Loss: 2.8558, Batch Time: 59.27s\n",
      "Epoch 1, Batch 234/500, Loss: 2.9222, Batch Time: 58.34s\n",
      "Epoch 1, Batch 235/500, Loss: 3.0299, Batch Time: 59.17s\n",
      "Epoch 1, Batch 236/500, Loss: 3.2128, Batch Time: 62.25s\n",
      "Epoch 1, Batch 237/500, Loss: 3.1879, Batch Time: 1201.30s\n",
      "Epoch 1, Batch 238/500, Loss: 2.8861, Batch Time: 4926.55s\n",
      "Epoch 1, Batch 239/500, Loss: 3.2561, Batch Time: 2194.26s\n",
      "Epoch 1, Batch 240/500, Loss: 3.0982, Batch Time: 2310.75s\n",
      "Epoch 1, Batch 241/500, Loss: 3.2225, Batch Time: 4001.42s\n",
      "Epoch 1, Batch 242/500, Loss: 3.2295, Batch Time: 3664.71s\n",
      "Epoch 1, Batch 243/500, Loss: 2.9325, Batch Time: 6159.32s\n",
      "Epoch 1, Batch 244/500, Loss: 2.9725, Batch Time: 911.46s\n",
      "Epoch 1, Batch 245/500, Loss: 3.2483, Batch Time: 2526.91s\n",
      "Epoch 1, Batch 246/500, Loss: 3.2497, Batch Time: 2464.67s\n",
      "Epoch 1, Batch 247/500, Loss: 2.8967, Batch Time: 2028.71s\n",
      "Epoch 1, Batch 248/500, Loss: 3.0862, Batch Time: 2731.65s\n",
      "Epoch 1, Batch 249/500, Loss: 2.9711, Batch Time: 1554.22s\n",
      "Epoch 1, Batch 250/500, Loss: 2.9548, Batch Time: 512.62s\n",
      "Epoch 1, Batch 251/500, Loss: 3.0758, Batch Time: 223.69s\n",
      "Epoch 1, Batch 252/500, Loss: 3.0771, Batch Time: 156.32s\n",
      "Epoch 1, Batch 253/500, Loss: 2.9803, Batch Time: 944.14s\n",
      "Epoch 1, Batch 254/500, Loss: 3.0631, Batch Time: 756.83s\n",
      "Epoch 1, Batch 255/500, Loss: 3.0881, Batch Time: 61.05s\n",
      "Epoch 1, Batch 256/500, Loss: 2.9519, Batch Time: 62.65s\n",
      "Epoch 1, Batch 257/500, Loss: 3.0813, Batch Time: 59.17s\n",
      "Epoch 1, Batch 258/500, Loss: 2.9208, Batch Time: 59.62s\n",
      "Epoch 1, Batch 259/500, Loss: 3.2965, Batch Time: 70.56s\n",
      "Epoch 1, Batch 260/500, Loss: 3.4095, Batch Time: 79.38s\n",
      "Epoch 1, Batch 261/500, Loss: 2.9435, Batch Time: 66.53s\n",
      "Epoch 1, Batch 262/500, Loss: 3.3555, Batch Time: 60.94s\n",
      "Epoch 1, Batch 263/500, Loss: 3.1746, Batch Time: 60.60s\n",
      "Epoch 1, Batch 264/500, Loss: 2.9106, Batch Time: 65.57s\n",
      "Epoch 1, Batch 265/500, Loss: 3.4082, Batch Time: 65.00s\n",
      "Epoch 1, Batch 266/500, Loss: 2.9708, Batch Time: 60.58s\n",
      "Epoch 1, Batch 267/500, Loss: 3.1344, Batch Time: 65.29s\n",
      "Epoch 1, Batch 268/500, Loss: 2.9756, Batch Time: 63.71s\n",
      "Epoch 1, Batch 269/500, Loss: 3.1641, Batch Time: 68.47s\n",
      "Epoch 1, Batch 270/500, Loss: 3.2388, Batch Time: 61.64s\n",
      "Epoch 1, Batch 271/500, Loss: 3.3460, Batch Time: 61.25s\n",
      "Epoch 1, Batch 272/500, Loss: 2.9593, Batch Time: 65.34s\n",
      "Epoch 1, Batch 273/500, Loss: 3.2667, Batch Time: 70.61s\n",
      "Epoch 1, Batch 274/500, Loss: 2.9354, Batch Time: 69.47s\n",
      "Epoch 1, Batch 275/500, Loss: 3.0532, Batch Time: 68.84s\n",
      "Epoch 1, Batch 276/500, Loss: 3.1160, Batch Time: 66.72s\n",
      "Epoch 1, Batch 277/500, Loss: 3.0870, Batch Time: 65.00s\n",
      "Epoch 1, Batch 278/500, Loss: 3.0553, Batch Time: 65.74s\n",
      "Epoch 1, Batch 279/500, Loss: 3.0982, Batch Time: 62.92s\n",
      "Epoch 1, Batch 280/500, Loss: 3.2777, Batch Time: 70.55s\n",
      "Epoch 1, Batch 281/500, Loss: 3.1042, Batch Time: 66.65s\n",
      "Epoch 1, Batch 282/500, Loss: 3.1342, Batch Time: 66.75s\n",
      "Epoch 1, Batch 283/500, Loss: 2.8589, Batch Time: 67.29s\n",
      "Epoch 1, Batch 284/500, Loss: 2.9555, Batch Time: 72.21s\n",
      "Epoch 1, Batch 285/500, Loss: 3.0081, Batch Time: 64.42s\n",
      "Epoch 1, Batch 286/500, Loss: 3.2208, Batch Time: 67.57s\n",
      "Epoch 1, Batch 287/500, Loss: 2.9810, Batch Time: 60.16s\n",
      "Epoch 1, Batch 288/500, Loss: 3.2599, Batch Time: 68.01s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 157\u001b[0m\n\u001b[1;32m    154\u001b[0m src, tgt \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), tgt\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m output \u001b[38;5;241m=\u001b[39m model(src, tgt[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m    158\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), tgt[\u001b[38;5;241m1\u001b[39m:, :]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    159\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m loss  \u001b[38;5;66;03m# Scale the loss by half\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 112\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m    109\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(tgt)\n\u001b[1;32m    111\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(tgt\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(src, tgt, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask)\n\u001b[1;32m    113\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(output)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:278\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     )\n\u001b[1;32m    272\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    273\u001b[0m     src,\n\u001b[1;32m    274\u001b[0m     mask\u001b[38;5;241m=\u001b[39msrc_mask,\n\u001b[1;32m    275\u001b[0m     src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask,\n\u001b[1;32m    276\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39msrc_is_causal,\n\u001b[1;32m    277\u001b[0m )\n\u001b[0;32m--> 278\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m    279\u001b[0m     tgt,\n\u001b[1;32m    280\u001b[0m     memory,\n\u001b[1;32m    281\u001b[0m     tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask,\n\u001b[1;32m    282\u001b[0m     memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    283\u001b[0m     tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    284\u001b[0m     memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[1;32m    285\u001b[0m     tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal,\n\u001b[1;32m    286\u001b[0m     memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal,\n\u001b[1;32m    287\u001b[0m )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:602\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    599\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 602\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(\n\u001b[1;32m    603\u001b[0m         output,\n\u001b[1;32m    604\u001b[0m         memory,\n\u001b[1;32m    605\u001b[0m         tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask,\n\u001b[1;32m    606\u001b[0m         memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    607\u001b[0m         tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    608\u001b[0m         memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask,\n\u001b[1;32m    609\u001b[0m         tgt_is_causal\u001b[38;5;241m=\u001b[39mtgt_is_causal,\n\u001b[1;32m    610\u001b[0m         memory_is_causal\u001b[38;5;241m=\u001b[39mmemory_is_causal,\n\u001b[1;32m    611\u001b[0m     )\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    614\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1091\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1086\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m   1087\u001b[0m         x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal)\n\u001b[1;32m   1088\u001b[0m     )\n\u001b[1;32m   1089\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(\n\u001b[1;32m   1090\u001b[0m         x\n\u001b[0;32m-> 1091\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(\n\u001b[1;32m   1092\u001b[0m             x, memory, memory_mask, memory_key_padding_mask, memory_is_causal\n\u001b[1;32m   1093\u001b[0m         )\n\u001b[1;32m   1094\u001b[0m     )\n\u001b[1;32m   1095\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:1127\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mha_block\u001b[39m(\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1121\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1126\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1127\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultihead_attn(\n\u001b[1;32m   1128\u001b[0m         x,\n\u001b[1;32m   1129\u001b[0m         mem,\n\u001b[1;32m   1130\u001b[0m         mem,\n\u001b[1;32m   1131\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1132\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1133\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1134\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1135\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1343\u001b[0m         query,\n\u001b[1;32m   1344\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1369\u001b[0m         query,\n\u001b[1;32m   1370\u001b[0m         key,\n\u001b[1;32m   1371\u001b[0m         value,\n\u001b[1;32m   1372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[1;32m   1373\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1374\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight,\n\u001b[1;32m   1375\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k,\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v,\n\u001b[1;32m   1378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1381\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1382\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1383\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1384\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m   1385\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1386\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1387\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1388\u001b[0m     )\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:6278\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6275\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m   6276\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m-> 6278\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m scaled_dot_product_attention(\n\u001b[1;32m   6279\u001b[0m     q, k, v, attn_mask, dropout_p, is_causal\n\u001b[1;32m   6280\u001b[0m )\n\u001b[1;32m   6281\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   6282\u001b[0m     attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[1;32m   6283\u001b[0m )\n\u001b[1;32m   6285\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset (only first 5000 rows)\n",
    "final_df = pd.read_csv('final_chess_games.csv', nrows=5000)\n",
    "final_df = final_df[['Result', 'AN']]\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(df):\n",
    "    sequences = df['AN'].apply(lambda x: x.strip().split())\n",
    "    return sequences.tolist()\n",
    "\n",
    "print(\"Preprocessing data...\")\n",
    "sequences = preprocess_data(final_df)\n",
    "print(\"Data preprocessing completed.\")\n",
    "\n",
    "# Build vocabulary\n",
    "all_moves = set(move for seq in sequences for move in seq)\n",
    "vocab = {move: idx for idx, move in enumerate(all_moves)}\n",
    "vocab['<pad>'] = len(vocab)  # Add padding token\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_length = max(len(seq) for seq in sequences) - 1\n",
    "\n",
    "# Define the dataset\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, sequences, vocab, max_length):\n",
    "        self.sequences = sequences\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_seq = [self.vocab.get(move, self.vocab['<pad>']) for move in sequence[:-1]]\n",
    "        target_seq = [self.vocab.get(move, self.vocab['<pad>']) for move in sequence[1:]]\n",
    "\n",
    "        # Pad sequences\n",
    "        input_seq += [self.vocab['<pad>']] * (self.max_length - len(input_seq))\n",
    "        target_seq += [self.vocab['<pad>']] * (self.max_length - len(target_seq))\n",
    "\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "train_sequences, val_sequences = train_test_split(sequences, test_size=0.2, random_state=42)\n",
    "train_dataset = ChessDataset(train_sequences, vocab, max_length)\n",
    "val_dataset = ChessDataset(val_sequences, vocab, max_length)\n",
    "print(\"Datasets created.\")\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 8  # Adjust batch size if necessary\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"Data loaders initialized.\")\n",
    "\n",
    "# Define positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(-math.log(10000.0) * torch.arange(0, d_model, 2).float() / d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "        output = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_layers = 6\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "print(\"Initializing model, loss function, and optimizer...\")\n",
    "model = TransformerModel(vocab_size, d_model, nhead, num_layers, dim_feedforward, dropout)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"Model, loss function, and optimizer initialized.\")\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10\n",
    "checkpoint_interval = 2  # Save checkpoint every 2 epochs\n",
    "accumulation_steps = 4  # Number of batches to accumulate gradients\n",
    "patience = 3  # Early stopping patience\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for batch_idx, (src, tgt) in enumerate(train_loader):\n",
    "        batch_start_time = time.time()\n",
    "        src, tgt = src.transpose(0, 1).to(device), tgt.transpose(0, 1).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(src, tgt[:-1, :])\n",
    "        loss = criterion(output.view(-1, vocab_size), tgt[1:, :].reshape(-1))\n",
    "        loss = 0.5 * loss  # Scale the loss by half\n",
    "\n",
    "        # Backward pass\n",
    "        (loss / accumulation_steps).backward()\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch_elapsed_time = time.time() - batch_start_time\n",
    "        print(f\"Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}, Batch Time: {batch_elapsed_time:.2f}s\")\n",
    "\n",
    "    epoch_elapsed_time = time.time() - epoch_start_time\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} training completed. Average Loss: {avg_loss:.4f}. Elapsed Time: {epoch_elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Validation step\n",
    "    validation_start_time = time.time()\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.transpose(0, 1).to(device), tgt.transpose(0, 1).to(device)\n",
    "\n",
    "            output = model(src, tgt[:-1, :])\n",
    "            loss = criterion(output.view(-1, vocab_size), tgt[1:, :].reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    validation_elapsed_time = time.time() - validation_start_time\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}. Validation Time: {validation_elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Validation loss improved. Model saved.\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"No improvement in validation loss for {counter} epoch(s).\")\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        checkpoint_path = f'transformer_model_epoch_{epoch+1}.pth'\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "print(\"Training completed and best model loaded.\")\n",
    "\n",
    "# Function to generate moves\n",
    "def generate_moves(model, start_sequence, vocab, max_length=50):\n",
    "    model.eval()\n",
    "    generated_moves = start_sequence.copy()\n",
    "    input_seq = [vocab.get(move, vocab['<pad>']) for move in generated_moves]\n",
    "    input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(1).to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        tgt_input = input_tensor[-1:, :]\n",
    "        output = model(input_tensor, tgt_input)\n",
    "        next_move_idx = output.argmax(dim=-1)[-1, 0].item()\n",
    "        next_move = [move for move, idx in vocab.items() if idx == next_move_idx][0]\n",
    "        generated_moves.append(next_move)\n",
    "\n",
    "        if next_move == '<pad>':\n",
    "            break\n",
    "\n",
    "        input_seq.append(next_move_idx)\n",
    "        input_tensor = torch.tensor(input_seq, dtype=torch.long).unsqueeze(1).to(device)\n",
    "\n",
    "    return generated_moves\n",
    "\n",
    "# Example usage\n",
    "start_sequence = ['1.', 'e4', 'e5', '2.', 'Nf3', 'Nc6']\n",
    "generated_moves = generate_moves(model, start_sequence, vocab)\n",
    "print(\"Generated moves:\", ' '.join(generated_moves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
