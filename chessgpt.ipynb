{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":10071159,"sourceType":"datasetVersion","datasetId":6207515}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#data\n#https://www.kaggle.com/datasets/milesh1/35-million-chess-games?resource=download","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom transformers import BertTokenizer, Trainer, TrainingArguments\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import BertForSequenceClassification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T23:16:13.404898Z","iopub.execute_input":"2024-12-01T23:16:13.405525Z","iopub.status.idle":"2024-12-01T23:16:57.997593Z","shell.execute_reply.started":"2024-12-01T23:16:13.405487Z","shell.execute_reply":"2024-12-01T23:16:57.996528Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1733095010.735303      13 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:479\nE1201 23:16:50.774956573      13 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-12-01T23:16:50.774939574+00:00\", grpc_status:2}\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Step 1: Dataset Parsing and Preprocessing\ndef parse_dataset(file_path):\n    \"\"\"\n    Parse the dataset to extract metadata and moves.\n    \"\"\"\n    games = []\n    count =0\n    with open(file_path, 'r') as f:\n        for line in f:\n            if \"###\" in line:\n                parts = line.strip().split(\"###\")\n                metadata = parts[0].split()\n                moves = parts[1].strip()\n                if count > 1000:\n                    break\n                if count%100==0:\n                    print(count)\n                count=count+1\n                # Extract metadata\n                game_id = int(metadata[0])\n                date = metadata[1]\n                result = metadata[2]\n                white_elo = int(metadata[3]) if metadata[3] != \"None\" else None\n                black_elo = int(metadata[4]) if metadata[4] != \"None\" else None\n                num_moves = int(metadata[5])\n                \n                # Append game data\n                games.append({\n                    \"game_id\": game_id,\n                    \"date\": date,\n                    \"result\": result,\n                    \"white_elo\": white_elo,\n                    \"black_elo\": black_elo,\n                    \"num_moves\": num_moves,\n                    \"moves\": moves\n                })\n    return pd.DataFrame(games)\n\ndef preprocess_moves(move_sequence):\n    \"\"\"\n    Extract moves from the sequence.\n    \"\"\"\n    moves = re.findall(r'[WB]\\d+\\.[a-hRNBQKxO-]+\\+?#?', move_sequence)\n    return [move[3:] for move in moves]\n\ndef create_sequences(moves, result):\n    \"\"\"\n    Create input-output pairs for model training.\n    \"\"\"\n    result_label = 1 if result == \"1-0\" else 0 if result == \"0-1\" else 2  # White win, Black win, Draw\n    sequences = []\n    for i in range(1, len(moves)):\n        input_seq = moves[:i]\n        next_move = moves[i]\n        sequences.append((\" \".join(input_seq), next_move, result_label))\n    return sequences\n\n# Load dataset\nfile_path = \"/kaggle/input/chessgptdata/all_with_filtered_anotations_since1998.txt\"\ndf = parse_dataset(file_path)\n\n# Preprocess move sequences\ndf['moves'] = df['moves'].apply(preprocess_moves)\n\n# Generate input-output pairs\nsequences = []\nfor _, row in df.iterrows():\n    sequences.extend(create_sequences(row['moves'], row['result']))\n\nseq_df = pd.DataFrame(sequences, columns=[\"input_sequence\", \"next_move\", \"result_label\"])\n\n# Save processed data\nseq_df.to_csv(\"processed_chess_data.csv\", index=False)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T23:17:00.988554Z","iopub.execute_input":"2024-12-01T23:17:00.989626Z","iopub.status.idle":"2024-12-01T23:17:01.609621Z","shell.execute_reply.started":"2024-12-01T23:17:00.989540Z","shell.execute_reply":"2024-12-01T23:17:01.608386Z"}},"outputs":[{"name":"stdout","text":"0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Step 2: Dataset Class for PyTorch\nclass ChessDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        item = self.data.iloc[index]\n        input_seq = item['input_sequence']\n        next_move = item['next_move']  # Keep as a string\n        result_label = int(item['result_label'])\n    \n        # Tokenize input sequence\n        encoding = self.tokenizer(\n            input_seq,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n    \n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'labels': self.tokenizer.convert_tokens_to_ids(next_move),  # Convert move to token ID\n            'result_label': torch.tensor(result_label, dtype=torch.long)\n        }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T23:17:05.867244Z","iopub.execute_input":"2024-12-01T23:17:05.867643Z","iopub.status.idle":"2024-12-01T23:17:05.874522Z","shell.execute_reply.started":"2024-12-01T23:17:05.867609Z","shell.execute_reply":"2024-12-01T23:17:05.873645Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\n# Step 3: Train-Test Split\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nMAX_LEN = 50\n\ntrain_data, val_data = train_test_split(seq_df, test_size=0.1, random_state=42)\ntrain_dataset = ChessDataset(train_data, tokenizer, MAX_LEN)\nval_dataset = ChessDataset(val_data, tokenizer, MAX_LEN)\n\n# Step 4: Custom Model\nclass ChessPredictionModel(torch.nn.Module):\n    def __init__(self, num_moves, num_results):\n        super(ChessPredictionModel, self).__init__()\n        self.bert = BertForSequenceClassification.from_pretrained(\n            'bert-base-uncased',\n            num_labels=num_moves,\n            output_hidden_states=True  # Enable hidden states\n        )\n        self.result_classifier = torch.nn.Linear(768, num_results)\n        self.loss_fn = torch.nn.CrossEntropyLoss()  # Use CrossEntropyLoss for classification tasks\n\n    def forward(self, input_ids, attention_mask, labels=None, result_label=None):\n        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        logits_moves = bert_output.logits\n\n        # Ensure hidden states are available\n        if bert_output.hidden_states is not None:\n            pooled_output = bert_output.hidden_states[-1][:, 0]\n            logits_result = self.result_classifier(pooled_output)\n        else:\n            raise ValueError(\"Hidden states are not available in the BERT output.\")\n\n        output = {\n            \"logits_moves\": logits_moves,\n            \"logits_result\": logits_result,\n        }\n\n        if labels is not None and result_label is not None:\n            # Compute the loss for both predictions\n            move_loss = self.loss_fn(logits_moves, labels)\n            result_loss = self.loss_fn(logits_result, result_label)\n            total_loss = move_loss + result_loss\n            output[\"loss\"] = total_loss\n\n        return output\n\n\n\n# Instantiate the model\nnum_moves = len(seq_df['next_move'].unique())\nnum_results = 3  # White win, Black win, Draw\nmodel = ChessPredictionModel(num_moves=num_moves, num_results=num_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T23:17:07.183959Z","iopub.execute_input":"2024-12-01T23:17:07.184949Z","iopub.status.idle":"2024-12-01T23:17:11.191156Z","shell.execute_reply.started":"2024-12-01T23:17:07.184909Z","shell.execute_reply":"2024-12-01T23:17:11.190229Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Step 5: Training with Checkpointing\ntraining_args = TrainingArguments(\n    output_dir='./checkpoints',\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=50,\n    evaluation_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=3,\n    learning_rate=5e-5,\n    load_best_model_at_end=True\n)\n\n# Define evaluation metrics\ndef compute_metrics(pred):\n    logits_moves, logits_result = pred.predictions\n    labels_moves, labels_result = pred.label_ids\n    move_preds = torch.argmax(torch.tensor(logits_moves), dim=-1)\n    result_preds = torch.argmax(torch.tensor(logits_result), dim=-1)\n\n    move_accuracy = accuracy_score(labels_moves, move_preds.numpy())\n    result_accuracy = accuracy_score(labels_result, result_preds.numpy())\n    return {\"move_accuracy\": move_accuracy, \"result_accuracy\": result_accuracy}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)\n\n# Train with checkpointing\ntrainer.train(resume_from_checkpoint=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T23:17:17.666268Z","iopub.execute_input":"2024-12-01T23:17:17.666683Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1733095039.257597      13 common_lib.cc:818] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:483\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='2620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   6/2620 01:58 < 21:26:52, 0.03 it/s, Epoch 0.01/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Step 6: Save Model\nmodel.save_pretrained(\"./chess_model\")\ntokenizer.save_pretrained(\"./chess_model\")\n\n# Step 7: Evaluation\neval_results = trainer.evaluate()\nprint(\"Evaluation Results:\", eval_results)\n\n# Step 8: Test Prediction\ndef predict_next_move(model, tokenizer, input_moves):\n    model.eval()\n    input_ids = tokenizer(\n        input_moves,\n        return_tensors='pt',\n        padding='max_length',\n        truncation=True,\n        max_length=MAX_LEN\n    )['input_ids']\n\n    with torch.no_grad():\n        logits_moves, logits_result = model(input_ids)\n        predicted_move = torch.argmax(logits_moves, dim=-1).item()\n        predicted_result = torch.argmax(logits_result, dim=-1).item()\n\n    return predicted_move, predicted_result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Test Example\ntest_moves = \"W1.e4 B1.e5 W2.Nf3\"\npredicted_move, predicted_result = predict_next_move(model, tokenizer, test_moves)\nprint(f\"Predicted Move: {predicted_move}, Predicted Result: {predicted_result}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}